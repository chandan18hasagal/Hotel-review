{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "%matplotlib inline\n",
    "\n",
    "import re, io ,regex\n",
    "import spacy\n",
    "import en_core_web_lg\n",
    "import textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviewid</th>\n",
       "      <th>Hotelid</th>\n",
       "      <th>userid</th>\n",
       "      <th>Date</th>\n",
       "      <th>reviewtext</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Review_1</td>\n",
       "      <td>hotel_101</td>\n",
       "      <td>hotel_1608</td>\n",
       "      <td>Nov 16, 2007</td>\n",
       "      <td>Nice Marriot       View of my king bed room</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Review_2</td>\n",
       "      <td>hotel_101</td>\n",
       "      <td>hotel_6939</td>\n",
       "      <td>Oct 30, 2007</td>\n",
       "      <td>Good hotel, charges for internet access The Ma...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Review_3</td>\n",
       "      <td>hotel_101</td>\n",
       "      <td>hotel_3976</td>\n",
       "      <td>Oct 12, 2007</td>\n",
       "      <td>Small but adequate rooms If you have an early ...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Review_4</td>\n",
       "      <td>hotel_101</td>\n",
       "      <td>hotel_2851</td>\n",
       "      <td>Aug 31, 2007</td>\n",
       "      <td>Better than average, some noisy rooms I have s...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Review_5</td>\n",
       "      <td>hotel_101</td>\n",
       "      <td>hotel_7897</td>\n",
       "      <td>Jul 18, 2007</td>\n",
       "      <td>Ordinary Although it is highly rated in these ...</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Reviewid    Hotelid      userid          Date  \\\n",
       "0  Review_1  hotel_101  hotel_1608  Nov 16, 2007   \n",
       "1  Review_2  hotel_101  hotel_6939  Oct 30, 2007   \n",
       "2  Review_3  hotel_101  hotel_3976  Oct 12, 2007   \n",
       "3  Review_4  hotel_101  hotel_2851  Aug 31, 2007   \n",
       "4  Review_5  hotel_101  hotel_7897  Jul 18, 2007   \n",
       "\n",
       "                                          reviewtext Sentiment  \n",
       "0        Nice Marriot       View of my king bed room      good  \n",
       "1  Good hotel, charges for internet access The Ma...      good  \n",
       "2  Small but adequate rooms If you have an early ...      good  \n",
       "3  Better than average, some noisy rooms I have s...      good  \n",
       "4  Ordinary Although it is highly rated in these ...       bad  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"C:/Users/sohan/Downloads/PHD_ML/Data/Train.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviewid</th>\n",
       "      <th>Hotelid</th>\n",
       "      <th>userid</th>\n",
       "      <th>Date</th>\n",
       "      <th>reviewtext</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6287</td>\n",
       "      <td>6287</td>\n",
       "      <td>6287</td>\n",
       "      <td>6287</td>\n",
       "      <td>6287</td>\n",
       "      <td>6287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>6287</td>\n",
       "      <td>100</td>\n",
       "      <td>6100</td>\n",
       "      <td>1310</td>\n",
       "      <td>6287</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Review_3326</td>\n",
       "      <td>hotel_188</td>\n",
       "      <td>hotel_4370</td>\n",
       "      <td>May 29, 2007</td>\n",
       "      <td>'Perplexed, Oblivious, and Clueless' quoted th...</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>521</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>3213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Reviewid    Hotelid      userid          Date  \\\n",
       "count          6287       6287        6287          6287   \n",
       "unique         6287        100        6100          1310   \n",
       "top     Review_3326  hotel_188  hotel_4370  May 29, 2007   \n",
       "freq              1        521           4            21   \n",
       "\n",
       "                                               reviewtext Sentiment  \n",
       "count                                                6287      6287  \n",
       "unique                                               6287         3  \n",
       "top     'Perplexed, Oblivious, and Clueless' quoted th...      good  \n",
       "freq                                                    1      3213  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Reviewid', 'Hotelid', 'userid', 'Date', 'reviewtext', 'Sentiment'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Sentiment']=pd.Categorical(data['Sentiment'],['bad','good','excellent'],ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewtext=data.reviewtext\n",
    "target=data.Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          Nice Marriot       View of my king bed room\n",
      "1    Good hotel, charges for internet access The Ma...\n",
      "2    Small but adequate rooms If you have an early ...\n",
      "3    Better than average, some noisy rooms I have s...\n",
      "4    Ordinary Although it is highly rated in these ...\n",
      "Name: reviewtext, dtype: object\n",
      "0    good\n",
      "1    good\n",
      "2    good\n",
      "3    good\n",
      "4     bad\n",
      "Name: Sentiment, dtype: category\n",
      "Categories (3, object): [bad < good < excellent]\n",
      "(6287,) (6287,)\n"
     ]
    }
   ],
   "source": [
    "print(reviewtext.head())\n",
    "print(target.head())\n",
    "print(reviewtext.shape,target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Sanitising text__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Changing the text into lower case\n",
    "reviewtext = reviewtext.apply(lambda x:(textacy.preprocess.preprocess_text(x,lowercase=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing all numbers, emailid, currency with str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewtext = reviewtext.apply(lambda x:(textacy.preprocess.replace_numbers(x, replace_with=\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewtext = reviewtext.apply(lambda x:(textacy.preprocess.replace_emails(x, replace_with=\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewtext = reviewtext.apply(lambda x:(textacy.preprocess.replace_currency_symbols(x, replace_with=\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What are your expectations? Your enjoyment at The Mosser depends greatly on your expectation. This is not to mean that it must be lowered to appreciate the stay. Know what The Mosser is, and arrive with the knowledge that it isn't a  -Star modern resort with tile walls and spacious rooms adjoining rooms, nor is it a carbon copy of hotel chains that deliver mundane and mindless comfort. This is a charming little hotel, in every sense of the adjective. The small rooms contain sparten but modern and functional furnitures. The chair in the corner has an artistic swivel table top that you can pull over your lap comfortbly and work on your laptop, using the wireless internet. The lights are modern decor that retains functionality. The beds, although not the kinds you'd find in a mansion, are neat and comfortable. The in-room bathroom is clean and decently lighted, while the hall bathrooms are equally spotless -- the only difference is that they are in the hallway (I had my own in-room bathroom, but also noted that I rarely see the hall bathrooms used; and they're separated by genders with locks).The full-length mirror in my room was useful for me to see how aweful my black linen shirt matched my yellow cargo shorts. The pictures on the wall are deco and the TV, while not once turned on, was installed up on the wall, out of the way yet easy to view. This is the city. And what's more, the Mosser is centrally located in the city. Lots of energy rising from the street below (I had a street-side room), and shopping galore. You'll see people of all type, and at last-call-for-alcohol (if you're up that late) you'll hear some louder people stumbling back to wherever it is they came from, or into cabs. Few and far in between. In other words, it is a busy street that has the sound of any big city. However, the quadruple-pane glass windows are immensely helpful at shutting the world out of your sleep. A great and important value you can't ignore!The restaurant down stairs, Annabelle's Bar and Bistro, serves up a great seared ahi tuna dish that begs you to ask the question, When did I die and go to heaven? One word of advice, avoid the corner room immediately above the restaurant.A few doors down is a great organic coffee shop, walking distance in your flip flops. I love the wine country, adore the beaches, and can't get enough of the islands; but, nothing gives me more energy and excitement than staying in an environment almost entirely man-made. And The Mosser is definitely a great choice to check into. The prices are reasonable and the accomodation is functionally fun and charming. So, know where you're going and set your expectation accordingly. You wouldn't expect an apple to taste like an orange, would you?Regards.[If you have questions that I didn't cover, feel free to contact me:   ]\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For email\n",
    "reviewtext[2520]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Great Value Just spent two nights at the Pointe Hilton Squaw Peak (Jan 2 and 3), we had arrived in Phoenix early due to medical appointment and tried checking in rather early (9 AM by hotel standards), had no problem. Housekeeping wasn't completely finished doing our suite so we had breakfast at one of their on-site restaurants and toured their facility (they have a great water park and activity area for kids). Our suite was comfortably appointed and unlike many hotels, clean and fresh smelling (you know what I mean). The king size bed was a pleasure to sleep in, the sheets seem to have a 500 thread count (very soft and luxurious). Our evening meal at their Lantana Grill was adequate. I golfed at their sister resort nearby Lookout Mountain Golf Course great course, my wife stayed at the pool area with the kids, they had a blast. The service from the staff was friendly and courteous; overall we felt that our experience was above average for this class of hotel and very good value for the rate that we paid ($130/night). We will definately stay here again if the opportunity should arise.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.reviewtext[548]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Great Value Just spent two nights at the Pointe Hilton Squaw Peak (Jan   and  ), we had arrived in Phoenix early due to medical appointment and tried checking in rather early (  AM by hotel standards), had no problem. Housekeeping wasn't completely finished doing our suite so we had breakfast at one of their on-site restaurants and toured their facility (they have a great water park and activity area for kids). Our suite was comfortably appointed and unlike many hotels, clean and fresh smelling (you know what I mean). The king size bed was a pleasure to sleep in, the sheets seem to have a   thread count (very soft and luxurious). Our evening meal at their Lantana Grill was adequate. I golfed at their sister resort nearby Lookout Mountain Golf Course great course, my wife stayed at the pool area with the kids, they had a blast. The service from the staff was friendly and courteous; overall we felt that our experience was above average for this class of hotel and very good value for the rate that we paid (  /night). We will definately stay here again if the opportunity should arise.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Currency and number replaced \"\"\n",
    "reviewtext[548]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Manual cleaning__\n",
    "\n",
    "replacing string \"showreview\" and \"full\" with \"\" <br>\n",
    "showReview(NUMBER,'full'); is a noise in 193 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['showReview(', \"'full');\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ t for t in reviewtext[24].split() if (t.startswith('showReview'))|(t.endswith(\";\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2325    [showReview(]\n",
       "3379    [showReview(]\n",
       "1868    [showReview(]\n",
       "5987    [showReview(]\n",
       "5988    [showReview(]\n",
       "1861    [showReview(]\n",
       "1860    [showReview(]\n",
       "1859    [showReview(]\n",
       "1858    [showReview(]\n",
       "1857    [showReview(]\n",
       "1855    [showReview(]\n",
       "1854    [showReview(]\n",
       "1364    [showReview(]\n",
       "1365    [showReview(]\n",
       "1366    [showReview(]\n",
       "260     [showReview(]\n",
       "2848    [showReview(]\n",
       "3378    [showReview(]\n",
       "3380    [showReview(]\n",
       "4088    [showReview(]\n",
       "3381    [showReview(]\n",
       "309     [showReview(]\n",
       "308     [showReview(]\n",
       "307     [showReview(]\n",
       "306     [showReview(]\n",
       "305     [showReview(]\n",
       "5655    [showReview(]\n",
       "5654    [showReview(]\n",
       "5653    [showReview(]\n",
       "5652    [showReview(]\n",
       "            ...      \n",
       "4195               []\n",
       "4196               []\n",
       "4197               []\n",
       "4198               []\n",
       "4199               []\n",
       "4179               []\n",
       "4177               []\n",
       "4155               []\n",
       "4165               []\n",
       "4156               []\n",
       "4157               []\n",
       "4158               []\n",
       "4159               []\n",
       "4160               []\n",
       "4161               []\n",
       "4162               []\n",
       "4163               []\n",
       "4164               []\n",
       "4166               []\n",
       "4176               []\n",
       "4167               []\n",
       "4168               []\n",
       "4169               []\n",
       "4170               []\n",
       "4171               []\n",
       "4172               []\n",
       "4173               []\n",
       "4174               []\n",
       "4175               []\n",
       "0                  []\n",
       "Name: reviewtext, Length: 6287, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(reviewtext.apply(lambda x:re.findall(r\"\\b[s][h][o][w][rR]\\S+\", x))).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewtext=reviewtext.apply(lambda x:re.sub(r\"\\b[s][h][o][w][rR]\\S+\",\"\", x))# replacing string \"showreview\" with \"\"\n",
    "reviewtext=reviewtext.apply(lambda x:re.sub(r\"\\b[f][u][l][l][']\\S+\",\"\", x))# replacing string \"full\" with \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Kind of worn out!    , '\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviewtext[24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481    [(http, www.youtube.com, /watch?v=tXK37DF1-CQb...\n",
       "2850    [(http, www.kuletos.com, /cafe/cafekuletos.sht...\n",
       "2327              [(http, virtualbizweb.com, /?p=23Just)]\n",
       "6286                                                   []\n",
       "2097                                                   []\n",
       "2089                                                   []\n",
       "Name: reviewtext, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding url strings\n",
    "((reviewtext.apply(lambda x:re.findall(r\"(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?\", x))).sort_values(ascending=False)).head(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Worst Service Ever I'll start with the positives: this is a great location close to all San Francisco's best attractions. Within walking distance to convenience stores, museums, Fisherman's Wharf and Pier  , cable car stops, and downtown.My problem here was bad service which I complained about all the way to Marriott Corporate. Our entire stay, the front desk was manned maybe once the whole time. Had payment problems (not our problem, and not one we ever experienced at another Marriott) at check-in which were compounded by a rude clerk.Cell phones would not work in building, which was nobody's fault. But when our family tried to contact us by calling the hotel number, though, nobody would answer the phone to patch them through for our entire trip just about! We had a death of a friend while we were staying here... we did not know about it because our family could not get anyone at the hotel to answer the phone!!Had I not been interested in collecting my Marriott Rewards points, we would have got a refund and checked into the Holiday Inn across the street. We actually contemplated it, and talked to the Holiday Inn staff (who were much nicer) but decided to tough it out since it was just a  -day trip.Not a fun anniversary though. I really LOVE Marriott 鈥?usually 鈥?but this property was horrendous simply because of the staff. Maybe it has changed since then, or was just an isolated string of coincidences, but I would not recommend this hotel based on my experience.This place was so bad, I even talked about it in a blog, LOLhttp://virtualbizweb.com/?p=23Just my two cents.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviewtext[2327]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reviewtext=reviewtext.apply(lambda x:re.sub(r\"(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?\",\"\", x))# replacing url strings with \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Worst Service Ever I'll start with the positives: this is a great location close to all San Francisco's best attractions. Within walking distance to convenience stores, museums, Fisherman's Wharf and Pier  , cable car stops, and downtown.My problem here was bad service which I complained about all the way to Marriott Corporate. Our entire stay, the front desk was manned maybe once the whole time. Had payment problems (not our problem, and not one we ever experienced at another Marriott) at check-in which were compounded by a rude clerk.Cell phones would not work in building, which was nobody's fault. But when our family tried to contact us by calling the hotel number, though, nobody would answer the phone to patch them through for our entire trip just about! We had a death of a friend while we were staying here... we did not know about it because our family could not get anyone at the hotel to answer the phone!!Had I not been interested in collecting my Marriott Rewards points, we would have got a refund and checked into the Holiday Inn across the street. We actually contemplated it, and talked to the Holiday Inn staff (who were much nicer) but decided to tough it out since it was just a  -day trip.Not a fun anniversary though. I really LOVE Marriott 鈥?usually 鈥?but this property was horrendous simply because of the staff. Maybe it has changed since then, or was just an isolated string of coincidences, but I would not recommend this hotel based on my experience.This place was so bad, I even talked about it in a blog, LOL my two cents.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviewtext[2327]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5319                                            [麓, 麓, 麓]\n",
       "5674                                               [麓, 麓]\n",
       "2854                                         [麓, 鈥, 檛, 鈥]\n",
       "5167                                                  [麓]\n",
       "47                                                    [麓]\n",
       "3610                                                  [霉]\n",
       "4749                                                  [鈧]\n",
       "2217                                                  [鈧]\n",
       "152                                    [鈥, 鈥, 鈥, 鈥, 鈥, 鈥]\n",
       "2596                                      [鈥, 鈥, 鈥, 鈥, 鈥]\n",
       "4388                                [鈥, 鈥, 鈥, 鈥, 檛, 鈥, 鈥]\n",
       "3928                                [鈥, 鈥, 鈥, 渂, 鈥, 鈥, 檇]\n",
       "3262        [鈥, 鈥, 鈥, 檝, 鈥, 檛, 鈥, 鈥, 檚, 鈥, 鈥, 鈥, 檛, 鈥, 檒]\n",
       "4841                       [鈥, 鈥, 鈥, 檛, 鈥, 鈥, 鈥, 鈥, 鈥, 檇]\n",
       "4267                    [鈥, 鈥, 鈥, 檚, 鈥, 鈥, 檚, 鈥, 檒, 鈥, 檇]\n",
       "5377                                [鈥, 鈥, 鈥, 檚, 鈥, 檚, 鈥]\n",
       "4256                                            [鈥, 鈥, 鈥]\n",
       "5333    [鈥, 鈥, 渇, 鈥, 鈥, 鈥, 鈥, 鈥, 鈥, 鈥, 渋, 鈥, 檛, 鈥, 鈥, ...\n",
       "1899                                         [鈥, 鈥, 渇, 鈥]\n",
       "1275                                         [鈥, 鈥, 渂, 鈥]\n",
       "4603                       [鈥, 鈥, 渁, 鈥, 鈥, 鈥, 淚, 鈥, 榃, 鈥]\n",
       "6268                                   [鈥, 鈥, 檝, 鈥, 鈥, 檛]\n",
       "870                                    [鈥, 鈥, 檛, 鈥, 鈥, 檛]\n",
       "4109                             [鈥, 鈥, 檛, 鈥, 檛, 陆, 鈥, 檛]\n",
       "601                                             [鈥, 鈥, 檛]\n",
       "2267    [鈥, 鈥, 檚, 鈥, 檛, 鈥, 鈥, 檛, 鈥, 檙, 鈥, 渞, 鈥, 鈥, 鈥, ...\n",
       "4366    [鈥, 鈥, 檚, 鈥, 檛, 鈥, 檛, 鈥, 檛, 鈥, 淭, 鈥, 檛, 鈥, 鈥, ...\n",
       "4787    [鈥, 鈥, 檚, 鈥, 檚, 鈥, 檛, 鈥, 檚, 鈥, 檚, 陆, 鈥, 檚, 鈥, ...\n",
       "1876                          [鈥, 鈥, 檙, 鈥, 檇, 鈥, 檙, 鈥, 檒]\n",
       "3052    [鈥, 鈥, 檇, 鈥, 檚, 鈥, 檚, 鈥, 渃, 鈥, 鈥, 檚, 鈥, 淰, 鈥, ...\n",
       "                              ...                        \n",
       "4184                                                   []\n",
       "4185                                                   []\n",
       "4187                                                   []\n",
       "4188                                                   []\n",
       "4189                                                   []\n",
       "4168                                                   []\n",
       "4166                                                   []\n",
       "4144                                                   []\n",
       "4154                                                   []\n",
       "4145                                                   []\n",
       "4146                                                   []\n",
       "4147                                                   []\n",
       "4148                                                   []\n",
       "4149                                                   []\n",
       "4150                                                   []\n",
       "4151                                                   []\n",
       "4152                                                   []\n",
       "4153                                                   []\n",
       "4155                                                   []\n",
       "4165                                                   []\n",
       "4156                                                   []\n",
       "4157                                                   []\n",
       "4158                                                   []\n",
       "4159                                                   []\n",
       "4160                                                   []\n",
       "4161                                                   []\n",
       "4162                                                   []\n",
       "4163                                                   []\n",
       "4164                                                   []\n",
       "0                                                      []\n",
       "Name: reviewtext, Length: 6287, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(reviewtext.apply(lambda x:re.findall(u'[⺀-⺙⺛-⻳⼀-⿕々〇〡-〩〸-〺〻㐀-䶵一-鿃豈-鶴侮-頻並-龎]', x))).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'great location, quiet and nice hotel We really liked the location of this hotel 鈥?on North Point Street, near the activity of the marina but not right in the thick of it. It鈥檚 a very quiet hotel. We had neighbors, but I never heard them once. Nice heated pool and hot tub. The rooms are decorated in a blue and brown modern theme, which we liked very much but which wouldn鈥檛 appeal to everyone. Only a few downsides for us, one of which was the bathroom 鈥?it was beautiful, but the sliding doors didn鈥檛 close completely or lock, and the shower was only half enclosed, meaning that you鈥檙e in there and the back half of the shower was open. This led to water getting out and it being a little colder because the steam could escape. Also, the shower head was a 鈥渞ain鈥?type shower head from the ceiling. Very nice, except that you have to reach under it to turn the water on, so you got a cold blast to the head when you reach forward. And one other picky thing 鈥?the ice bucket is so tiny that we couldn鈥檛 have two cans of soda in the ice at the same time. So one of us had to drink it warm. I asked for a second can and they said sure, but it never turned up. I didn鈥檛 follow up. And finally, the internet access was awful.    for a day, and you have to connect through the television. The keyboard was awful, you had to POUND on the keys to get them to type anything and it was totally frustrating. I spent about an hour online to do what would take five minutes at home. And the reason we got the access was to check my husband鈥檚 work e-mail, which we couldn鈥檛 do anyway since it wasn鈥檛 a secure enough connection. And if you bring a laptop, you can鈥檛 use it for internet in the room anyway. We didn鈥檛 check out the sports bar because there were so many other great places to eat and drink nearby. But, for the main things 鈥?the location and the room, it was great. The bed was fantastic, as were the pillows, and the people were very nice. I鈥檇 stay here again for sure.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviewtext[2267]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing chinese characters with \"\"\n",
    "reviewtext=reviewtext.apply(lambda x:re.sub(u'[⺀-⺙⺛-⻳⼀-⿕々〇〡-〩〸-〺〻㐀-䶵一-鿃豈-鶴侮-頻並-龎]',\"\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'great location, quiet and nice hotel We really liked the location of this hotel ?on North Point Street, near the activity of the marina but not right in the thick of it. It a very quiet hotel. We had neighbors, but I never heard them once. Nice heated pool and hot tub. The rooms are decorated in a blue and brown modern theme, which we liked very much but which wouldn appeal to everyone. Only a few downsides for us, one of which was the bathroom ?it was beautiful, but the sliding doors didn close completely or lock, and the shower was only half enclosed, meaning that youe in there and the back half of the shower was open. This led to water getting out and it being a little colder because the steam could escape. Also, the shower head was a ain?type shower head from the ceiling. Very nice, except that you have to reach under it to turn the water on, so you got a cold blast to the head when you reach forward. And one other picky thing ?the ice bucket is so tiny that we couldn have two cans of soda in the ice at the same time. So one of us had to drink it warm. I asked for a second can and they said sure, but it never turned up. I didn follow up. And finally, the internet access was awful.    for a day, and you have to connect through the television. The keyboard was awful, you had to POUND on the keys to get them to type anything and it was totally frustrating. I spent about an hour online to do what would take five minutes at home. And the reason we got the access was to check my husband work e-mail, which we couldn do anyway since it wasn a secure enough connection. And if you bring a laptop, you can use it for internet in the room anyway. We didn check out the sports bar because there were so many other great places to eat and drink nearby. But, for the main things ?the location and the room, it was great. The bed was fantastic, as were the pillows, and the people were very nice. I stay here again for sure.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviewtext[2267]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2072    [................................................\n",
       "4760                        [...........................]\n",
       "4000                              [.....................]\n",
       "2207                                        [...........]\n",
       "273                                           [.........]\n",
       "1392                                            [.......]\n",
       "2104                                            [.......]\n",
       "1494                                             [......]\n",
       "223                                               [.....]\n",
       "90                                                [.....]\n",
       "359                                               [.....]\n",
       "4853                                              [.....]\n",
       "4519                                              [.....]\n",
       "3267                                              [.....]\n",
       "3976                                               [....]\n",
       "4135                                               [....]\n",
       "1621                                               [....]\n",
       "1319                                               [....]\n",
       "5692                                               [....]\n",
       "5743                                               [....]\n",
       "1104                                               [....]\n",
       "400                                                [....]\n",
       "566                                                [....]\n",
       "3036                                               [....]\n",
       "1266                                               [....]\n",
       "5736                                               [....]\n",
       "5998                                               [....]\n",
       "1076                                               [....]\n",
       "3815                                               [....]\n",
       "2383                                               [....]\n",
       "                              ...                        \n",
       "3113                                                   []\n",
       "3111                                                   []\n",
       "3107                                                   []\n",
       "3106                                                   []\n",
       "3103                                                   []\n",
       "3100                                                   []\n",
       "3094                                                   []\n",
       "3158                                                   []\n",
       "3159                                                   []\n",
       "3160                                                   []\n",
       "3198                                                   []\n",
       "3235                                                   []\n",
       "3230                                                   []\n",
       "3228                                                   []\n",
       "3224                                                   []\n",
       "3218                                                   []\n",
       "3210                                                   []\n",
       "3204                                                   []\n",
       "3201                                                   []\n",
       "3197                                                   []\n",
       "3162                                                   []\n",
       "3196                                                   []\n",
       "3194                                                   []\n",
       "3181                                                   []\n",
       "3179                                                   []\n",
       "3178                                                   []\n",
       "3175                                                   []\n",
       "3173                                                   []\n",
       "3170                                                   []\n",
       "0                                                      []\n",
       "Name: reviewtext, Length: 6287, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(reviewtext.apply(lambda x:re.findall(\"(\\.+)$\", x))).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing strings like \".....\" with \".\"\n",
    "reviewtext=reviewtext.apply(lambda x:re.sub(\"(\\.+)$\",\".\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "761     [.]\n",
       "3353    [.]\n",
       "927     [.]\n",
       "926     [.]\n",
       "3349    [.]\n",
       "3350    [.]\n",
       "3351    [.]\n",
       "3352    [.]\n",
       "925     [.]\n",
       "3328    [.]\n",
       "3355    [.]\n",
       "3356    [.]\n",
       "924     [.]\n",
       "923     [.]\n",
       "3359    [.]\n",
       "3360    [.]\n",
       "3346    [.]\n",
       "3345    [.]\n",
       "3344    [.]\n",
       "928     [.]\n",
       "3342    [.]\n",
       "3341    [.]\n",
       "3340    [.]\n",
       "3339    [.]\n",
       "3338    [.]\n",
       "3337    [.]\n",
       "3336    [.]\n",
       "3335    [.]\n",
       "3334    [.]\n",
       "929     [.]\n",
       "       ... \n",
       "3113     []\n",
       "3111     []\n",
       "3107     []\n",
       "3106     []\n",
       "3103     []\n",
       "3100     []\n",
       "3094     []\n",
       "3158     []\n",
       "3159     []\n",
       "3160     []\n",
       "3198     []\n",
       "3235     []\n",
       "3230     []\n",
       "3228     []\n",
       "3224     []\n",
       "3218     []\n",
       "3210     []\n",
       "3204     []\n",
       "3201     []\n",
       "3197     []\n",
       "3162     []\n",
       "3196     []\n",
       "3194     []\n",
       "3181     []\n",
       "3179     []\n",
       "3178     []\n",
       "3175     []\n",
       "3173     []\n",
       "3170     []\n",
       "0        []\n",
       "Name: reviewtext, Length: 6287, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(reviewtext.apply(lambda x:re.findall(\"(\\.+)$\", x))).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5253              [stays       Bathroom]\n",
       "100                   [stay       lobby]\n",
       "2338                   [stay       View]\n",
       "1298               [situacion       bao]\n",
       "4988                      [shop  Subway]\n",
       "5676                  [room       Hotel]\n",
       "1509                 [quiet  restaurant]\n",
       "2263                [property       Gym]\n",
       "632             [location       outside]\n",
       "3547           [location       Exterior]\n",
       "2059                 [hotel       lobby]\n",
       "3812              [hotel       entrance]\n",
       "3111                [hotel       Shower]\n",
       "5953                 [hotel       Hotel]\n",
       "1247                   [every   minutes]\n",
       "2547               [confort       Faade]\n",
       "2777                 [at       Bathroom]\n",
       "4859                      [about   mins]\n",
       "4801                    [US       table]\n",
       "5123               [Stay       entrance]\n",
       "4157            [Sofitel       bathroom]\n",
       "3054                    [Room   Bedroom]\n",
       "2228        [Renovation       Guestroom]\n",
       "1218                   [Park       Sofa]\n",
       "2156                    [Nice  spacious]\n",
       "2336           [Marriott       enterior]\n",
       "4508        [Location          Fountain]\n",
       "4219    [Konfortabel       Doppelzimmer]\n",
       "2524                  [Hotel       room]\n",
       "4233              [Hotel       bathroom]\n",
       "                      ...               \n",
       "4212                                  []\n",
       "4213                                  []\n",
       "4214                                  []\n",
       "4215                                  []\n",
       "4216                                  []\n",
       "4195                                  []\n",
       "4194                                  []\n",
       "4193                                  []\n",
       "4181                                  []\n",
       "4172                                  []\n",
       "4173                                  []\n",
       "4174                                  []\n",
       "4175                                  []\n",
       "4176                                  []\n",
       "4177                                  []\n",
       "4178                                  []\n",
       "4179                                  []\n",
       "4180                                  []\n",
       "4182                                  []\n",
       "4192                                  []\n",
       "4183                                  []\n",
       "4184                                  []\n",
       "4185                                  []\n",
       "4186                                  []\n",
       "4187                                  []\n",
       "4188                                  []\n",
       "4189                                  []\n",
       "4190                                  []\n",
       "4191                                  []\n",
       "0                                     []\n",
       "Name: reviewtext, Length: 6287, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(reviewtext.apply(lambda x:re.findall(\"(\\w+\\s+\\s+\\w+)$\", x))).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing strings like double space \"    \" with \" \"\n",
    "reviewtext=reviewtext.apply(lambda x:re.sub(\"(\\w+\\s+\\s+\\w+)$\",\"\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6286    []\n",
       "2087    []\n",
       "2089    []\n",
       "2090    []\n",
       "2091    []\n",
       "2092    []\n",
       "2093    []\n",
       "2094    []\n",
       "2095    []\n",
       "2096    []\n",
       "2097    []\n",
       "2098    []\n",
       "2099    []\n",
       "2100    []\n",
       "2101    []\n",
       "2102    []\n",
       "2103    []\n",
       "2104    []\n",
       "2105    []\n",
       "2106    []\n",
       "2107    []\n",
       "2108    []\n",
       "2109    []\n",
       "2088    []\n",
       "2086    []\n",
       "1964    []\n",
       "2085    []\n",
       "2064    []\n",
       "2065    []\n",
       "2066    []\n",
       "        ..\n",
       "4217    []\n",
       "4218    []\n",
       "4219    []\n",
       "4220    []\n",
       "4221    []\n",
       "4200    []\n",
       "4199    []\n",
       "4198    []\n",
       "4186    []\n",
       "4177    []\n",
       "4178    []\n",
       "4179    []\n",
       "4180    []\n",
       "4181    []\n",
       "4182    []\n",
       "4183    []\n",
       "4184    []\n",
       "4185    []\n",
       "4187    []\n",
       "4197    []\n",
       "4188    []\n",
       "4189    []\n",
       "4190    []\n",
       "4191    []\n",
       "4192    []\n",
       "4193    []\n",
       "4194    []\n",
       "4195    []\n",
       "4196    []\n",
       "0       []\n",
       "Name: reviewtext, Length: 6287, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(reviewtext.apply(lambda x:re.findall(\"(\\w+\\s+\\s+\\w+)$\", x))).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewtext = reviewtext.apply(lambda x:(textacy.preprocess.remove_punct(x, marks=';:/)}(@{*#-_~|')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Great home base for Seattle sighseeing!! My Husband and I spent a long weekend at the Summerfield suites before an Alaska cruise. The hotel was a steal at   a night. (Hotwire) The hotel is with in walking to pikes market and the space needle. We did use the free hotel shuttle to bring us to the waterfront for dinner. The drive staff were very friendly and got a great tip for dinner!! Elliott oyster bar, one of the best meals I have ever had!! The free breakfast was great!! All in All my stay in Seattle at the Summerfield suites was great!! Clean rooms, great views of the space needle and a friendly, helpful staff!! Would stay again in a heartbeat!!"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_reviewtext = reviewtext.apply(nlp)\n",
    "spacy_reviewtext[113]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Great home base for Seattle sighseeing!!,\n",
       " My Husband and I spent a long weekend at the Summerfield suites before an Alaska cruise.,\n",
       " The hotel was a steal at   a night.  ,\n",
       " Hotwire  ,\n",
       " The hotel is with in walking to pikes market and the space needle.,\n",
       " We did use the free hotel shuttle to bring us to the waterfront for dinner.,\n",
       " The drive staff were very friendly and got a great tip for dinner!!,\n",
       " Elliott oyster bar, one of the best meals I have ever had!!,\n",
       " The free breakfast was great!!,\n",
       " All in All my stay in Seattle at the Summerfield suites was great!!,\n",
       " Clean rooms, great views of the space needle and a friendly, helpful staff!!,\n",
       " Would stay again in a heartbeat!!]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(spacy_reviewtext[113].sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lowercased: 2010878274846645280 nickel\n",
      "lemma: 17836911535321982992 Nickel\n",
      "shape: 16072095006890171862 Xxxxx\n",
      "prefix: 10850957288794122991 N\n",
      "suffix: 18031763335440530313 kel\n",
      "log probability: -14.784602165222168\n",
      "Brown cluster id: 614\n",
      "----------------------------------------\n",
      "lowercased: 2283656566040971221 and\n",
      "lemma: 2283656566040971221 and\n",
      "shape: 4088098365541558500 xxx\n",
      "prefix: 11901859001352538922 a\n",
      "suffix: 2283656566040971221 and\n",
      "log probability: -4.113108158111572\n",
      "Brown cluster id: 20\n",
      "----------------------------------------\n",
      "lowercased: 16362895577960388672 dimed\n",
      "lemma: 11513532041974030651 Dimed\n",
      "shape: 16072095006890171862 Xxxxx\n",
      "prefix: 3929483993944067416 D\n",
      "suffix: 14009161575225144129 med\n",
      "log probability: -17.894472122192383\n",
      "Brown cluster id: 0\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i, token in enumerate(spacy_reviewtext[12]):\n",
    "    #print(\"original:\", token.orth, token.orth_)\n",
    "    print(\"lowercased:\", token.lower, token.lower_)\n",
    "    print(\"lemma:\", token.lemma, token.lemma_)\n",
    "    print(\"shape:\", token.shape, token.shape_)\n",
    "    print(\"prefix:\", token.prefix, token.prefix_)\n",
    "    print(\"suffix:\", token.suffix, token.suffix_)\n",
    "    print(\"log probability:\", token.prob)\n",
    "    print(\"Brown cluster id:\", token.cluster)\n",
    "    print(\"----------------------------------------\")\n",
    "    if i > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Great home base for Seattle sighseeing!! My Husband and I spent a long weekend at the Summerfield suites before an Alaska cruise. The hotel was a steal at   a night. (Hotwire) The hotel is with in walking to pikes market and the space needle. We did use the free hotel shuttle to bring us to the waterfront for dinner. The drive staff were very friendly and got a great tip for dinner!! Elliott oyster bar, one of the best meals I have ever had!! The free breakfast was great!! All in All my stay in Seattle at the Summerfield suites was great!! Clean rooms, great views of the space needle and a friendly, helpful staff!! Would stay again in a heartbeat!!"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_reviewtext[113]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Good hotel, charges for internet access,\n",
       " The Marriott Airport Seattle is a reasonable choice in the Sea Tac area.,\n",
       " I paid about    per night.,\n",
       " The shuttle is generally quick to and from the airport,\n",
       " and I encourage its use rather taxi service which can be very spotty in Seattle.,\n",
       " My room was in the second floor but in the back of the hotel such that I had a very long walk no matter which way I took to the room.,\n",
       " The interior is nice and check in was very quick.,\n",
       " Breakfast was good in the restaurant, if not a bit pricey.,\n",
       " Drink,\n",
       " service in the bar was fine, no complaints.,\n",
       " I can imagine that it must get very noisy in the lobby when the crowd is hanging out in thebar.,\n",
       " I guess Marriott has a new program where guests are now charged for internet service.,\n",
       " Its    at this hotel, but can be had for free at the business center near the check in counters.,\n",
       " There are two internet PCs and one boarding pass access,\n",
       " PC.Personally, the ability to sit in my room and check,\n",
       " my personal or work email is nice and I am not sure how many takers will pay extra for the access.,\n",
       " And I am hoping not to get nickel and dimed for that.,\n",
       " I can offer four stars as the overall quality was good.]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(spacy_reviewtext[1].sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    {96: 'PROPN', 103: 'SPACE', 85: 'ADP', 90: 'DE...\n",
       "1    {84: 'ADJ', 92: 'NOUN', 97: 'PUNCT', 85: 'ADP'...\n",
       "2    {84: 'ADJ', 89: 'CCONJ', 92: 'NOUN', 85: 'ADP'...\n",
       "3    {84: 'ADJ', 85: 'ADP', 97: 'PUNCT', 90: 'DET',...\n",
       "4    {84: 'ADJ', 85: 'ADP', 95: 'PRON', 100: 'VERB'...\n",
       "Name: reviewtext, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_pos_tag=spacy_reviewtext.apply(lambda x:({w.pos: w.pos_ for w in x}))\n",
    "review_pos_tag.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Lemmatize__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{5711639017775284443: 'good',\n",
       " 10491429878208300068: 'hotel',\n",
       " 2593208677638477497: ',',\n",
       " 16743499924604303818: 'charge',\n",
       " 16037325823156266367: 'for',\n",
       " 10937452440342651049: 'internet',\n",
       " 15459493437284569003: 'access',\n",
       " 7425985699627899538: 'the',\n",
       " 11090251104613371396: 'Marriott',\n",
       " 16083597864187794159: 'Airport',\n",
       " 715181609030382522: 'Seattle',\n",
       " 10382539506755952630: 'be',\n",
       " 11901859001352538922: 'a',\n",
       " 16093343421169014319: 'reasonable',\n",
       " 14385143700534068980: 'choice',\n",
       " 3002984154512732771: 'in',\n",
       " 291779009535289497: 'Sea',\n",
       " 17848402196536084053: 'Tac',\n",
       " 5961633188818984298: 'area',\n",
       " 12646065887601541794: '.',\n",
       " 561228191312463089: '-PRON-',\n",
       " 645224182615914541: 'pay',\n",
       " 942632335873952620: 'about',\n",
       " 6718839663412986256: '   ',\n",
       " 799935474406865112: 'per',\n",
       " 13511804770843510887: 'night',\n",
       " 8053403189179670421: 'shuttle',\n",
       " 18390186386827547778: 'generally',\n",
       " 12442504647632856847: 'quick',\n",
       " 3791531372978436496: 'to',\n",
       " 2283656566040971221: 'and',\n",
       " 7831658034963690409: 'from',\n",
       " 1289759116526216761: 'airport',\n",
       " 18244613377274813947: 'encourage',\n",
       " 6873750497785110593: 'use',\n",
       " 15549368236163590771: 'rather',\n",
       " 6824712009862111972: 'taxi',\n",
       " 208172016153456603: 'service',\n",
       " 7063653163634019529: 'which',\n",
       " 6635067063807956629: 'can',\n",
       " 9548244504980166557: 'very',\n",
       " 15667338818002046384: 'spotty',\n",
       " 14629044596807299988: 'room',\n",
       " 18193336062778529705: 'second',\n",
       " 3825921715910707007: 'floor',\n",
       " 14560795576765492085: 'but',\n",
       " 15255859468896132977: 'back',\n",
       " 886050111519832510: 'of',\n",
       " 13040105543478938413: 'such',\n",
       " 4380130941430378203: 'that',\n",
       " 14692702688101715474: 'have',\n",
       " 12965068231793614765: 'long',\n",
       " 1674876016505392235: 'walk',\n",
       " 13055779130471031426: 'no',\n",
       " 14513380027006169650: 'matter',\n",
       " 6878210874361030284: 'way',\n",
       " 6789454535283781228: 'take',\n",
       " 12510118381124562640: 'interior',\n",
       " 14121509715367036122: 'nice',\n",
       " 13320680580156776400: 'check',\n",
       " 12956416454023936912: 'breakfast',\n",
       " 13030476447309488184: 'restaurant',\n",
       " 12446819118446800910: 'if',\n",
       " 447765159362469301: 'not',\n",
       " 1794436035373472204: 'bit',\n",
       " 3658319564285253472: 'pricey',\n",
       " 8799542030263951485: 'drink',\n",
       " 5898049090513426128: 'bar',\n",
       " 16884455833306854589: 'fine',\n",
       " 14080437627224567055: 'complaint',\n",
       " 11158042576763907092: 'imagine',\n",
       " 7290638946010101875: 'must',\n",
       " 2013399242189103424: 'get',\n",
       " 5527247695594041383: 'noisy',\n",
       " 1431318213587614631: 'lobby',\n",
       " 15807309897752499399: 'when',\n",
       " 6077804502062178139: 'crowd',\n",
       " 4780549502391586051: 'hang',\n",
       " 1696981056005371314: 'out',\n",
       " 14045462305802064042: 'thebar',\n",
       " 1399273842756378753: 'guess',\n",
       " 4753564829687343602: 'new',\n",
       " 17812688126189747487: 'program',\n",
       " 16318918034475841628: 'where',\n",
       " 2752958001556593635: 'guest',\n",
       " 17157488710739566268: 'now',\n",
       " 11667289587015813222: 'at',\n",
       " 1995909169258310477: 'this',\n",
       " 17712913731469760764: 'free',\n",
       " 8928139183970412233: 'business',\n",
       " 14980700777765958138: 'center',\n",
       " 4578410152339589874: 'near',\n",
       " 15043103400775822501: 'counter',\n",
       " 2112642640949226496: 'there',\n",
       " 11711838292424000352: 'two',\n",
       " 4320100750554050543: 'pc',\n",
       " 17454115351911680600: 'one',\n",
       " 13085376474465521989: 'boarding',\n",
       " 4028211059174100358: 'pass',\n",
       " 5581275103926942631: 'pc.personally',\n",
       " 11565809527369121409: 'ability',\n",
       " 14192039007865877226: 'sit',\n",
       " 861244163374911113: 'personal',\n",
       " 3740602843040177340: 'or',\n",
       " 10038440415813069799: 'work',\n",
       " 7320900731437023467: 'email',\n",
       " 2985827712170516640: 'sure',\n",
       " 16331095434822636218: 'how',\n",
       " 9720044723474553187: 'many',\n",
       " 4074826125999842869: 'taker',\n",
       " 18307573501153647118: 'will',\n",
       " 6345425296343705333: 'extra',\n",
       " 4429974322456332988: 'hope',\n",
       " 2010878274846645280: 'nickel',\n",
       " 9028069759267288312: 'dim',\n",
       " 6502919589296598169: 'offer',\n",
       " 13283271314760746512: 'four',\n",
       " 230120506323693159: 'star',\n",
       " 7437575085468336610: 'as',\n",
       " 8578797347073582537: 'overall',\n",
       " 8617680374120575546: 'quality'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(spacy_reviewtext.apply(lambda x:({w.lemma: w.lemma_ for w in x})))[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Stop Words__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['your', 'before', 'something', 'even', 'thereupon', 'although', 'say', 'well', 'whence', 'thereafter']\n"
     ]
    }
   ],
   "source": [
    "#Importing stop words for English.\n",
    "\n",
    "stopwords_spacy_default= spacy.lang.en.stop_words.STOP_WORDS\n",
    "print(list(stopwords_spacy_default)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%time spacy_reviewtext_clean = spacy_reviewtext_lemma.apply(lambda x:[token.text for token in x if not(token.is_punct | token.is_space | token.is_stop)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.0768581e-02,  1.6200192e-01, -1.1659623e-01, -1.0489543e-01,\n",
       "        1.4206311e-01,  5.0967996e-04,  3.8513049e-02, -1.8854725e-01,\n",
       "        1.5390434e-02,  2.2249782e+00], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_reviewtext[1].vector[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the punctuations of string module\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "stopwords = list(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def spacy_tokenizer(sentence):\n",
    "    mytokens = parser(sentence)\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "    mytokens = [word.is_digit]\n",
    "    mytokens = [ word for word in mytokens if word not in stopwords and word not in punctuations ]\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STOPLIST = set(stopwords.words('english') + list(ENGLISH_STOP_WORDS))\n",
    "SYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-\", \"...\", \"”\", \"”\"]\n",
    "class CleanTextTransformer(TransformerMixin):\n",
    "   def transform(self, X, **transform_params):\n",
    "        return [cleanText(text) for text in X]\n",
    "   def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "def get_params(self, deep=True):\n",
    "        return {}\n",
    "    \n",
    "def cleanText(text):\n",
    "    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    text = text.lower()\n",
    "    return text\n",
    "def tokenizeText(sample):\n",
    "    tokens = parser(sample)\n",
    "    lemmas = []\n",
    "    for tok in tokens:\n",
    "        lemmas.append(tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_)\n",
    "    tokens = lemmas\n",
    "    tokens = [tok for tok in tokens if tok not in STOPLIST]\n",
    "    tokens = [tok for tok in tokens if tok not in SYMBOLS]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def printNMostInformative(vectorizer, clf, N):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    topClass1 = coefs_with_fns[:N]\n",
    "    topClass2 = coefs_with_fns[:-(N + 1):-1]\n",
    "    topClass3 = coefs_with_fns[:-(N + 2):-1]\n",
    "    print(\"Class 1 best: \")\n",
    "    for feat in topClass1:\n",
    "        print(feat)\n",
    "    print(\"Class 2 best: \")\n",
    "    for feat in topClass2:\n",
    "        print(feat)\n",
    "    print(\"Class 3 best: \")\n",
    "    for feat in topClass3:\n",
    "        print(feat)\n",
    "vectorizer = CountVectorizer(tokenizer=tokenizeText, ngram_range=(1,1))\n",
    "clf = LogisticRegression(solver='saga')\n",
    "\n",
    "pipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer), ('clf', clf)])\n",
    "# data\n",
    "\n",
    "train1 = X_train#.tolist()\n",
    "labelsTrain1 = y_train#.tolist()\n",
    "val1 = X_val#.tolist()\n",
    "labelsval1 = y_val#.tolist()\n",
    "# train\n",
    "pipe.fit(train1, labelsTrain1)\n",
    "# val\n",
    "preds = pipe.predict(val1)\n",
    "print(\"accuracy:\", accuracy_score(labelsval1, preds))\n",
    "print(\"Top 10 features used to predict: \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "printNMostInformative(vectorizer, clf, 20)\n",
    "pipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer)])\n",
    "transform = pipe.fit_transform(train1, labelsTrain1)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "for i in range(len(train1)):\n",
    "    s = \"\"\n",
    "    indexIntoVocab = transform.indices[transform.indptr[i]:transform.indptr[i+1]]\n",
    "    numOccurences = transform.data[transform.indptr[i]:transform.indptr[i+1]]\n",
    "    for idx, num in zip(indexIntoVocab, numOccurences):\n",
    "        s += str((vocab[idx], num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "punctuations = string.punctuation\n",
    "REPLACE_BY_SPACE_RE = re.compile(\"[/(){}\\[\\]\\|@,;'~]\")\n",
    "BAD_SYMBOLS_RE = re.compile('[^a-zA-Z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english')+ list(ENGLISH_STOP_WORDS)+list(STOP_WORDS))\n",
    "\n",
    "def cleaning_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "#text = textacy.preprocess.replace_numbers(text, replace_with=\" \")\n",
    "#text = textacy.preprocess.replace_emails(text, replace_with=\" \")\n",
    "#text = textacy.preprocess.replace_currency_symbols(text, replace_with=\" \")\n",
    "    text = re.sub(r\"\\b[s][h][o][w][rR]\\S+\",\"\", text)# replacing string \"showreview\" with \"\"\n",
    "    text = re.sub(r\"\\b[f][u][l][l][']\\S+\",\"\", text)# replacing string \"full\" with \"\"\n",
    "    text = re.sub(r\"(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?\",\"\", text)# replacing url strings  with \"\"\n",
    "    text = re.sub(u'[⺀-⺙⺛-⻳⼀-⿕々〇〡-〩〸-〺〻㐀-䶵一-鿃豈-鶴侮-頻並-龎]',\"\", text)\n",
    "    text = re.sub(\"(\\w+\\s+\\s+\\w+)$\",\"\", text)\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub('', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub(' ', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwords from text\n",
    "    return text\n",
    "parser=English()\n",
    "\n",
    "def spacy_tokenizer(sentence):\n",
    "  \n",
    "    mytokens = parser(sentence)\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "    mytokens = [ word for word in mytokens if word not in punctuations ]\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(doc):\n",
    "    return [x.orth_ for x in nlp(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewtext=reviewtext.apply(cleaning_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_val_split\n",
    "X = reviewtext\n",
    "y = target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3732)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=spacy_tokenizer,strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 1), use_idf=True,smooth_idf=True,sublinear_tf=True,\n",
    "            stop_words = 'english')\n",
    "# Fit and transform Tf-idf to both training and test sets\n",
    "tfidf.fit(list(X_train) + list(X_test))\n",
    "X_train_tfidf =  tfidf.transform(X_train) \n",
    "X_test_tfidf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "countvec = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 1), stop_words = 'english', binary=True)\n",
    "# Fit and transform CountVectorizer to both training and test sets\n",
    "countvec.fit(list(X_train) + list(X_test))\n",
    "X_train_countvec =  countvec.transform(X_train) \n",
    "X_test_countvec = countvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing, decomposition\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      " [[  3   1 212]\n",
      " [  0  11 374]\n",
      " [  0  10 647]]\n",
      "\n",
      "Val Set Accuracy Score :\n",
      " 0.5254372019077902\n",
      "\n",
      "Train Set Accuracy Score :\n",
      " 0.5516007158480811\n",
      "\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        bad       1.00      0.01      0.03       216\n",
      "  excellent       0.50      0.03      0.05       385\n",
      "       good       0.52      0.98      0.68       657\n",
      "\n",
      "avg / total       0.60      0.53      0.38      1258\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Feeding the data using tfidf\n",
    "classifier_tf_nb = MultinomialNB()\n",
    "classifier_tf_nb.fit(X_train_tfidf,y_train)\n",
    "#Predicting \n",
    "y_pred = classifier_tf_nb.predict(X_test_tfidf)\n",
    "yt_pred = classifier_tf_nb.predict(X_train_tfidf)\n",
    "#Analyzing\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "print(f'Confusion Matrix :\\n {cm}\\n')\n",
    "print(f'Val Set Accuracy Score :\\n {accuracy_score(y_test,y_pred)}\\n')\n",
    "print(f'Train Set Accuracy Score :\\n {accuracy_score(y_train,yt_pred)}\\n')\n",
    "print(f'Classification Report :\\n {classification_report(y_test,y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      " [[141   2  73]\n",
      " [  2 183 200]\n",
      " [ 25 113 519]]\n",
      "\n",
      "Val Set Accuracy Score :\n",
      " 0.6701112877583466\n",
      "\n",
      "Train Set Accuracy Score :\n",
      " 0.8538476834360708\n",
      "\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        bad       0.84      0.65      0.73       216\n",
      "  excellent       0.61      0.48      0.54       385\n",
      "       good       0.66      0.79      0.72       657\n",
      "\n",
      "avg / total       0.67      0.67      0.66      1258\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Feeding the data using Countvector\n",
    "classifier_cv_nb = MultinomialNB()\n",
    "classifier_cv_nb.fit(X_train_countvec,y_train)\n",
    "#Predicting \n",
    "y_pred = classifier_cv_nb.predict(X_test_countvec)\n",
    "yt_pred = classifier_cv_nb.predict(X_train_countvec)\n",
    "#Analyzing\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "print(f'Confusion Matrix :\\n {cm}\\n')\n",
    "print(f'Val Set Accuracy Score :\\n {accuracy_score(y_test,y_pred)}\\n')\n",
    "print(f'Train Set Accuracy Score :\\n {accuracy_score(y_train,yt_pred)}\\n')\n",
    "print(f'Classification Report :\\n {classification_report(y_test,y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      " [[181   5  30]\n",
      " [  9 275 101]\n",
      " [ 76 176 405]]\n",
      "\n",
      "Val Set Accuracy Score :\n",
      " 0.6844197138314785\n",
      "\n",
      "Train Set Accuracy Score :\n",
      " 0.8411214953271028\n",
      "\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        bad       0.68      0.84      0.75       216\n",
      "  excellent       0.60      0.71      0.65       385\n",
      "       good       0.76      0.62      0.68       657\n",
      "\n",
      "avg / total       0.70      0.68      0.68      1258\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Feeding the data using tfidf\n",
    "classifier_tv_lr = LogisticRegression(solver='saga',class_weight='balanced')\n",
    "classifier_tv_lr.fit(X_train_tfidf,y_train)\n",
    "#Predicting \n",
    "y_pred = classifier_tv_lr.predict(X_test_tfidf)\n",
    "yt_pred = classifier_tv_lr.predict(X_train_tfidf)\n",
    "#Analyzing\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "print(f'Confusion Matrix :\\n {cm}\\n')\n",
    "print(f'Val Set Accuracy Score :\\n {accuracy_score(y_test,y_pred)}\\n')\n",
    "print(f'Train Set Accuracy Score :\\n {accuracy_score(y_train,yt_pred)}\\n')\n",
    "print(f'Classification Report :\\n {classification_report(y_test,y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      " [[166   8  42]\n",
      " [  7 245 133]\n",
      " [ 48 177 432]]\n",
      "\n",
      "Val Set Accuracy Score :\n",
      " 0.6701112877583466\n",
      "\n",
      "Train Set Accuracy Score :\n",
      " 0.9554583416186121\n",
      "\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        bad       0.75      0.77      0.76       216\n",
      "  excellent       0.57      0.64      0.60       385\n",
      "       good       0.71      0.66      0.68       657\n",
      "\n",
      "avg / total       0.68      0.67      0.67      1258\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Feeding the data using Countvector\n",
    "classifier_cv_lr = LogisticRegression(solver='saga',class_weight='balanced')\n",
    "classifier_cv_lr.fit(X_train_countvec,y_train)\n",
    "#Predicting \n",
    "y_pred = classifier_cv_lr.predict(X_test_countvec)\n",
    "yt_pred = classifier_cv_lr.predict(X_train_countvec)\n",
    "#Analyzing\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "print(f'Confusion Matrix :\\n {cm}\\n')\n",
    "print(f'Val Set Accuracy Score :\\n {accuracy_score(y_test,y_pred)}\\n')\n",
    "print(f'Train Set Accuracy Score :\\n {accuracy_score(y_train,yt_pred)}\\n')\n",
    "print(f'Classification Report :\\n {classification_report(y_test,y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata_review=testdata_review.apply(cleaning_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "countvec = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 1), stop_words = 'english', binary=True)\n",
    "# Fit and transform CountVectorizer to both training and test sets\n",
    "countvec.fit(list(X_train) + list(X_test))\n",
    "\n",
    "test_countvec = countvec.transform(testdata_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=spacy_tokenizer,strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1,1), use_idf=True,smooth_idf=True,sublinear_tf=True,\n",
    "            stop_words = 'english')\n",
    "# Fit and transform Tf-idf to both training and test sets\n",
    "tfidf.fit(list(list(X_train) + list(X_test)))\n",
    "test_tfidf =  tfidf.transform(testdata_review) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5029, 16973)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3468, 16973)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['good', 'bad', 'bad', ..., 'good', 'good', 'excellent'],\n",
       "      dtype='<U9')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_cv_nb.predict(test_countvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_tv_lr.predict(test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission=pd.DataFrame(testdata.Reviewid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['Sentiment']=pd.Series(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviewid</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Review_11001</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Review_11002</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Review_11003</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Review_11004</td>\n",
       "      <td>excellent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Review_11005</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Review_11006</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Review_11007</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Review_11008</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Review_11009</td>\n",
       "      <td>excellent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Review_11010</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Review_11011</td>\n",
       "      <td>excellent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Review_11012</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Review_11013</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Review_11014</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Review_11015</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Review_11016</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Review_11017</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Review_11018</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Review_11019</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Review_11020</td>\n",
       "      <td>excellent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Review_11021</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Review_11022</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Review_11023</td>\n",
       "      <td>excellent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Review_11024</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Review_11025</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Review_11026</td>\n",
       "      <td>excellent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Review_11027</td>\n",
       "      <td>excellent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Review_11028</td>\n",
       "      <td>excellent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Review_11029</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Review_11030</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3439</th>\n",
       "      <td>Review_14440</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3440</th>\n",
       "      <td>Review_14441</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3441</th>\n",
       "      <td>Review_14442</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3442</th>\n",
       "      <td>Review_14443</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3443</th>\n",
       "      <td>Review_14444</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3444</th>\n",
       "      <td>Review_14445</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3445</th>\n",
       "      <td>Review_14446</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3446</th>\n",
       "      <td>Review_14447</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3447</th>\n",
       "      <td>Review_14448</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3448</th>\n",
       "      <td>Review_14449</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3449</th>\n",
       "      <td>Review_14450</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3450</th>\n",
       "      <td>Review_14451</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3451</th>\n",
       "      <td>Review_14452</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3452</th>\n",
       "      <td>Review_14453</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3453</th>\n",
       "      <td>Review_14454</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3454</th>\n",
       "      <td>Review_14455</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3455</th>\n",
       "      <td>Review_14456</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3456</th>\n",
       "      <td>Review_14457</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3457</th>\n",
       "      <td>Review_14458</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3458</th>\n",
       "      <td>Review_14459</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3459</th>\n",
       "      <td>Review_14460</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3460</th>\n",
       "      <td>Review_14461</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3461</th>\n",
       "      <td>Review_14462</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3462</th>\n",
       "      <td>Review_14463</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3463</th>\n",
       "      <td>Review_14464</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3464</th>\n",
       "      <td>Review_14465</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3465</th>\n",
       "      <td>Review_14466</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3466</th>\n",
       "      <td>Review_14467</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3467</th>\n",
       "      <td>Review_14468</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentiment</th>\n",
       "      <td>[bad, bad, good, excellent, good, bad, good, g...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3469 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Reviewid  Sentiment\n",
       "0                                               Review_11001        bad\n",
       "1                                               Review_11002        bad\n",
       "2                                               Review_11003       good\n",
       "3                                               Review_11004  excellent\n",
       "4                                               Review_11005       good\n",
       "5                                               Review_11006        bad\n",
       "6                                               Review_11007       good\n",
       "7                                               Review_11008       good\n",
       "8                                               Review_11009  excellent\n",
       "9                                               Review_11010       good\n",
       "10                                              Review_11011  excellent\n",
       "11                                              Review_11012        bad\n",
       "12                                              Review_11013        bad\n",
       "13                                              Review_11014       good\n",
       "14                                              Review_11015       good\n",
       "15                                              Review_11016       good\n",
       "16                                              Review_11017       good\n",
       "17                                              Review_11018       good\n",
       "18                                              Review_11019       good\n",
       "19                                              Review_11020  excellent\n",
       "20                                              Review_11021        bad\n",
       "21                                              Review_11022       good\n",
       "22                                              Review_11023  excellent\n",
       "23                                              Review_11024       good\n",
       "24                                              Review_11025        bad\n",
       "25                                              Review_11026  excellent\n",
       "26                                              Review_11027  excellent\n",
       "27                                              Review_11028  excellent\n",
       "28                                              Review_11029        bad\n",
       "29                                              Review_11030       good\n",
       "...                                                      ...        ...\n",
       "3439                                            Review_14440        NaN\n",
       "3440                                            Review_14441        NaN\n",
       "3441                                            Review_14442        NaN\n",
       "3442                                            Review_14443        NaN\n",
       "3443                                            Review_14444        NaN\n",
       "3444                                            Review_14445        NaN\n",
       "3445                                            Review_14446        NaN\n",
       "3446                                            Review_14447        NaN\n",
       "3447                                            Review_14448        NaN\n",
       "3448                                            Review_14449        NaN\n",
       "3449                                            Review_14450        NaN\n",
       "3450                                            Review_14451        NaN\n",
       "3451                                            Review_14452        NaN\n",
       "3452                                            Review_14453        NaN\n",
       "3453                                            Review_14454        NaN\n",
       "3454                                            Review_14455        NaN\n",
       "3455                                            Review_14456        NaN\n",
       "3456                                            Review_14457        NaN\n",
       "3457                                            Review_14458        NaN\n",
       "3458                                            Review_14459        NaN\n",
       "3459                                            Review_14460        NaN\n",
       "3460                                            Review_14461        NaN\n",
       "3461                                            Review_14462        NaN\n",
       "3462                                            Review_14463        NaN\n",
       "3463                                            Review_14464        NaN\n",
       "3464                                            Review_14465        NaN\n",
       "3465                                            Review_14466        NaN\n",
       "3466                                            Review_14467        NaN\n",
       "3467                                            Review_14468        NaN\n",
       "Sentiment  [bad, bad, good, excellent, good, bad, good, g...        NaN\n",
       "\n",
       "[3469 rows x 2 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building a pipeline\n",
    "#Naive Bayes\n",
    "classifier_nb = MultinomialNB()\n",
    "classifier_cv_nb = MultinomialNB()\n",
    "\n",
    "#Logistic regression\n",
    "classifier_lr = Pipeline([('vect',CountVectorizer(tokenizer=spacy_tokenizer, ngram_range=(1,2))),\n",
    "                          ('tfidf',TfidfVectorizer(ngram_range=(1,2))),('clf',LogisticRegression(solver='saga'))])\n",
    "classifier_cv_lr = Pipeline([('vect',CountVectorizer(tokenizer=spacy_tokenizer, ngram_range=(1,1))),('clf',LogisticRegression(solver='saga'))])\n",
    "\n",
    "#Random Forest\n",
    "classifier_tv_rf = Pipeline([('tfidf',TfidfVectorizer(tokenizer=spacy_tokenizer, ngram_range=(1,1))),('clf',RandomForestClassifier(bootstrap= False, criterion= 'entropy', n_estimators= 100))])\n",
    "classifier_cv_rf = Pipeline([('vect',CountVectorizer(tokenizer=spacy_tokenizer, ngram_range=(1,1))),('clf',RandomForestClassifier(bootstrap= False, criterion= 'entropy', n_estimators= 100))])\n",
    "\n",
    "#Linear SVC\n",
    "classifier_tv_svc = Pipeline([('tfidf',TfidfVectorizer(tokenizer=spacy_tokenizer, ngram_range=(1,1))),\n",
    "                     ('clf',LinearSVC())])\n",
    "classifier_cv_svc = Pipeline([('CountVector',CountVectorizer(tokenizer=spacy_tokenizer, ngram_range=(1,1))),\n",
    "                     ('clf',LinearSVC())])\n",
    "\n",
    "##K-Nearest Neighbour\n",
    "classifier_tv_knn = Pipeline([('tfidf',TfidfVectorizer(tokenizer=spacy_tokenizer, ngram_range=(1,1))),\n",
    "                     ('clf',KNeighborsClassifier())])\n",
    "classifier_cv_knn = Pipeline([('vect',CountVectorizer(tokenizer=spacy_tokenizer, ngram_range=(1,1))),\n",
    "                     ('clf',KNeighborsClassifier())])\n",
    "\n",
    "##XG Boost Neighbour\n",
    "classifier_tv_xgb = Pipeline([('tfidf',TfidfVectorizer(tokenizer=spacy_tokenizer, ngram_range=(1,1))),\n",
    "                     ('clf',xgb.XGBClassifier(max_depth=7,min_child_weight=1,learning_rate=0.1,n_estimators=500))])\n",
    "classifier_cv_xgb = Pipeline([('vect',CountVectorizer(tokenizer=spacy_tokenizer, ngram_range=(1,1))),\n",
    "                     ('clf',xgb.XGBClassifier(max_depth=7,min_child_weight=1,learning_rate=0.1,n_estimators=500))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Naive Bayes using TfidfVectorizer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-ba0a32d57741>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Feeding the data using tfidf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mclassifier_tv_nb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#Predicting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier_tv_nb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0myt_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier_tv_nb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    246\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m         \"\"\"\n\u001b[1;32m--> 248\u001b[1;33m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    211\u001b[0m                 Xt, fitted_transformer = fit_transform_one_cached(\n\u001b[0;32m    212\u001b[0m                     \u001b[0mcloned_transformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m                     **fit_params_steps[name])\n\u001b[0m\u001b[0;32m    214\u001b[0m                 \u001b[1;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                 \u001b[1;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, weight, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    579\u001b[0m                        **fit_params):\n\u001b[0;32m    580\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit_transform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1379\u001b[0m             \u001b[0mTf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0midf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1380\u001b[0m         \"\"\"\n\u001b[1;32m-> 1381\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1382\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1383\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 869\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    870\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 792\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    793\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 266\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "#Feeding the data using tfidf\n",
    "classifier_tv_nb.fit(X_train,y_train)\n",
    "#Predicting \n",
    "y_pred = classifier_tv_nb.predict(X_val)\n",
    "yt_pred = classifier_tv_nb.predict(X_train)\n",
    "#Analyzing\n",
    "cm = confusion_matrix(y_val,y_pred)\n",
    "print(f'Confusion Matrix :\\n {cm}\\n')\n",
    "print(f'Val Set Accuracy Score :\\n {accuracy_score(y_val,y_pred)}\\n')\n",
    "print(f'Train Set Accuracy Score :\\n {accuracy_score(y_train,yt_pred)}\\n')\n",
    "print(f'Classification Report :\\n {classification_report(y_val,y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Naive Bayes using CountVectorizer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "lower not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-10784b742a6d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Feeding the data using CountVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mclassifier_cv_nb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#Predicting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier_cv_nb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0myt_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier_cv_nb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    246\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m         \"\"\"\n\u001b[1;32m--> 248\u001b[1;33m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    211\u001b[0m                 Xt, fitted_transformer = fit_transform_one_cached(\n\u001b[0;32m    212\u001b[0m                     \u001b[0mcloned_transformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m                     **fit_params_steps[name])\n\u001b[0m\u001b[0;32m    214\u001b[0m                 \u001b[1;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                 \u001b[1;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, weight, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    579\u001b[0m                        **fit_params):\n\u001b[0;32m    580\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit_transform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1379\u001b[0m             \u001b[0mTf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0midf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1380\u001b[0m         \"\"\"\n\u001b[1;32m-> 1381\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1382\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1383\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 869\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    870\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 792\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    793\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 266\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    684\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    685\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" not found\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: lower not found"
     ]
    }
   ],
   "source": [
    "#Feeding the data using CountVectorizer\n",
    "classifier_cv_nb.fit(X_train,y_train)\n",
    "#Predicting \n",
    "y_pred = classifier_cv_nb.predict(X_val)\n",
    "yt_pred = classifier_cv_nb.predict(X_train)\n",
    "#Analyzing\n",
    "cm = confusion_matrix(y_val,y_pred)\n",
    "print(f'Confusion Matrix :\\n {cm}\\n')\n",
    "print(f'Val Set Accuracy Score :\\n {accuracy_score(y_val,y_pred)}\\n')\n",
    "print(f'Train Set Accuracy Score :\\n {accuracy_score(y_train,yt_pred)}\\n')\n",
    "print(f'Classification Report :\\n {classification_report(y_val,y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Logistic Regression TfidfVectorizer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "lower not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-7970d53a5a7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclassifier_tv_lr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#Predicting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier_tv_lr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0myt_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier_tv_lr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#Analyzing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    246\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m         \"\"\"\n\u001b[1;32m--> 248\u001b[1;33m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    211\u001b[0m                 Xt, fitted_transformer = fit_transform_one_cached(\n\u001b[0;32m    212\u001b[0m                     \u001b[0mcloned_transformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m                     **fit_params_steps[name])\n\u001b[0m\u001b[0;32m    214\u001b[0m                 \u001b[1;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                 \u001b[1;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, weight, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    579\u001b[0m                        **fit_params):\n\u001b[0;32m    580\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit_transform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1379\u001b[0m             \u001b[0mTf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0midf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1380\u001b[0m         \"\"\"\n\u001b[1;32m-> 1381\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1382\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1383\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 869\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    870\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 792\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    793\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 266\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    684\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    685\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" not found\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: lower not found"
     ]
    }
   ],
   "source": [
    "classifier_tv_lr.fit(X_train,y_train)\n",
    "#Predicting \n",
    "y_pred = classifier_tv_lr.predict(X_val)\n",
    "yt_pred = classifier_tv_lr.predict(X_train)\n",
    "#Analyzing\n",
    "cm = confusion_matrix(y_val,y_pred)\n",
    "print(f'Confusion Matrix :\\n {cm}\\n')\n",
    "print(f'Val Set Accuracy Score :\\n {accuracy_score(y_val,y_pred)}\\n')\n",
    "print(f'Train Set Accuracy Score :\\n {accuracy_score(y_train,yt_pred)}\\n')\n",
    "print(f'Classification Report :\\n {classification_report(y_val,y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Logistic Regression CountVectorizer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      " [[148  10  58]\n",
      " [  7 234 144]\n",
      " [ 29 130 498]]\n",
      "\n",
      "Val Set Accuracy Score :\n",
      " 0.699523052464229\n",
      "\n",
      "Train Set Accuracy Score :\n",
      " 0.7711274607277789\n",
      "\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        bad       0.80      0.69      0.74       216\n",
      "  excellent       0.63      0.61      0.62       385\n",
      "       good       0.71      0.76      0.73       657\n",
      "\n",
      "avg / total       0.70      0.70      0.70      1258\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier_cv_lr.fit(X_train,y_train)\n",
    "#Predicting \n",
    "y_pred = classifier_cv_lr.predict(X_val)\n",
    "yt_pred = classifier_cv_lr.predict(X_train)\n",
    "#Analyzing\n",
    "cm = confusion_matrix(y_val,y_pred)\n",
    "print(f'Confusion Matrix :\\n {cm}\\n')\n",
    "print(f'Val Set Accuracy Score :\\n {accuracy_score(y_val,y_pred)}\\n')\n",
    "print(f'Train Set Accuracy Score :\\n {accuracy_score(y_train,yt_pred)}\\n')\n",
    "print(f'Classification Report :\\n {classification_report(y_val,y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Random Forest TfidfVectorizer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      " [[ 85   3 128]\n",
      " [  1 114 270]\n",
      " [  8  52 597]]\n",
      "\n",
      "Val Set Accuracy Score :\n",
      " 0.6327503974562798\n",
      "\n",
      "Train Set Accuracy Score :\n",
      " 0.9990057665539869\n",
      "\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        bad       0.90      0.39      0.55       216\n",
      "  excellent       0.67      0.30      0.41       385\n",
      "       good       0.60      0.91      0.72       657\n",
      "\n",
      "avg / total       0.68      0.63      0.60      1258\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier_tv_rf.fit(X_train,y_train)\n",
    "y_pred = classifier_tv_rf.predict(X_val)\n",
    "yt_pred = classifier_tv_rf.predict(X_train)\n",
    "\n",
    "cm = confusion_matrix(y_val,y_pred)\n",
    "print(f'Confusion Matrix :\\n {cm}\\n')\n",
    "print(f'Val Set Accuracy Score :\\n {accuracy_score(y_val,y_pred)}\\n')\n",
    "print(f'Train Set Accuracy Score :\\n {accuracy_score(y_train,yt_pred)}\\n')\n",
    "print(f'Classification Report :\\n {classification_report(y_val,y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Random Forest CountVectorizer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      " [[148   6 192]\n",
      " [ 10 190 375]\n",
      " [ 28  96 842]]\n",
      "\n",
      "Test Set Accuracy Score :\n",
      " 0.6253312135665077\n",
      "\n",
      "Train Set Accuracy Score :\n",
      " 0.9995454545454545\n",
      "\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        bad       0.80      0.43      0.56       346\n",
      "  excellent       0.65      0.33      0.44       575\n",
      "       good       0.60      0.87      0.71       966\n",
      "\n",
      "avg / total       0.65      0.63      0.60      1887\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier_cv_rf.fit(X_train,y_train)\n",
    "y_pred = classifier_cv_rf.predict(X_val)\n",
    "yt_pred = classifier_cv_rf.predict(X_train)\n",
    "\n",
    "cm = confusion_matrix(y_val,y_pred)\n",
    "print(f'Confusion Matrix :\\n {cm}\\n')\n",
    "print(f'Val Set Accuracy Score :\\n {accuracy_score(y_val,y_pred)}\\n')\n",
    "print(f'Train Set Accuracy Score :\\n {accuracy_score(y_train,yt_pred)}\\n')\n",
    "print(f'Classification Report :\\n {classification_report(y_val,y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Linear SVC TfidfVectorizer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      " [[237  10  99]\n",
      " [ 11 305 259]\n",
      " [ 60 216 690]]\n",
      "\n",
      "Test Set Accuracy Score :\n",
      " 0.652888182299947\n",
      "\n",
      "Train Set Accuracy Score :\n",
      " 0.9711363636363637\n",
      "\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        bad       0.77      0.68      0.72       346\n",
      "  excellent       0.57      0.53      0.55       575\n",
      "       good       0.66      0.71      0.69       966\n",
      "\n",
      "avg / total       0.65      0.65      0.65      1887\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier_tv_svc.fit(X_train,y_train)\n",
    "y_pred = classifier_tv_svc.predict(X_val)\n",
    "yt_pred = classifier_tv_svc.predict(X_train)\n",
    "cm = confusion_matrix(y_val,y_pred)\n",
    "print(f'Confusion Matrix :\\n {cm}\\n')\n",
    "print(f'Val Set Accuracy Score :\\n {accuracy_score(y_val,y_pred)}\\n')\n",
    "print(f'Train Set Accuracy Score :\\n {accuracy_score(y_train,yt_pred)}\\n')\n",
    "print(f'Classification Report :\\n {classification_report(y_val,y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Linear SVC CountVectorizer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      " [[223  20 103]\n",
      " [ 13 304 258]\n",
      " [ 70 259 637]]\n",
      "\n",
      "Test Set Accuracy Score :\n",
      " 0.6168521462639109\n",
      "\n",
      "Train Set Accuracy Score :\n",
      " 0.9915909090909091\n",
      "\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        bad       0.73      0.64      0.68       346\n",
      "  excellent       0.52      0.53      0.53       575\n",
      "       good       0.64      0.66      0.65       966\n",
      "\n",
      "avg / total       0.62      0.62      0.62      1887\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier_cv_svc.fit(X_train,y_train)\n",
    "y_pred = classifier_cv_svc.predict(X_val)\n",
    "yt_pred = classifier_cv_svc.predict(X_train)\n",
    "cm = confusion_matrix(y_val,y_pred)\n",
    "print(f'Confusion Matrix :\\n {cm}\\n')\n",
    "print(f'Val Set Accuracy Score :\\n {accuracy_score(y_val,y_pred)}\\n')\n",
    "print(f'Train Set Accuracy Score :\\n {accuracy_score(y_train,yt_pred)}\\n')\n",
    "print(f'Classification Report :\\n {classification_report(y_val,y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__KNN TfidfVectorizer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      " [[138  64 144]\n",
      " [ 20 297 258]\n",
      " [ 65 341 560]]\n",
      "\n",
      "Test Set Accuracy Score :\n",
      " 0.5272919978802332\n",
      "\n",
      "Train Set Accuracy Score :\n",
      " 0.7181818181818181\n",
      "\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        bad       0.62      0.40      0.49       346\n",
      "  excellent       0.42      0.52      0.47       575\n",
      "       good       0.58      0.58      0.58       966\n",
      "\n",
      "avg / total       0.54      0.53      0.53      1887\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier_tv_knn.fit(X_train,y_train)\n",
    "y_pred = classifier_tv_knn.predict(X_val)\n",
    "yt_pred = classifier_tv_knn.predict(X_train)\n",
    "cm = confusion_matrix(y_val,y_pred)\n",
    "print(f'Confusion Matrix :\\n {cm}\\n')\n",
    "print(f'Val Set Accuracy Score :\\n {accuracy_score(y_val,y_pred)}\\n')\n",
    "print(f'Train Set Accuracy Score :\\n {accuracy_score(y_train,yt_pred)}\\n')\n",
    "print(f'Classification Report :\\n {classification_report(y_val,y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__KNN CountVectorizer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      " [[ 64 173 109]\n",
      " [ 19 336 220]\n",
      " [ 49 428 489]]\n",
      "\n",
      "Test Set Accuracy Score :\n",
      " 0.47111817700052994\n",
      "\n",
      "Train Set Accuracy Score :\n",
      " 0.6486363636363637\n",
      "\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        bad       0.48      0.18      0.27       346\n",
      "  excellent       0.36      0.58      0.44       575\n",
      "       good       0.60      0.51      0.55       966\n",
      "\n",
      "avg / total       0.50      0.47      0.47      1887\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifier_cv_knn.fit(X_train,y_train)\n",
    "y_pred = classifier_cv_knn.predict(X_val)\n",
    "yt_pred = classifier_cv_knn.predict(X_train)\n",
    "cm = confusion_matrix(y_val,y_pred)\n",
    "print(f'Confusion Matrix :\\n {cm}\\n')\n",
    "print(f'Val Set Accuracy Score :\\n {accuracy_score(y_val,y_pred)}\\n')\n",
    "print(f'Train Set Accuracy Score :\\n {accuracy_score(y_train,yt_pred)}\\n')\n",
    "print(f'Classification Report :\\n {classification_report(y_val,y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__XGB TfidfVectorizer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 49s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      " [[226   6 114]\n",
      " [ 10 283 282]\n",
      " [ 37 168 761]]\n",
      "\n",
      "Test Set Accuracy Score :\n",
      " 0.6730259671436142\n",
      "\n",
      "Train Set Accuracy Score :\n",
      " 0.990909090909091\n",
      "\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        bad       0.83      0.65      0.73       346\n",
      "  excellent       0.62      0.49      0.55       575\n",
      "       good       0.66      0.79      0.72       966\n",
      "\n",
      "avg / total       0.68      0.67      0.67      1887\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%time classifier_tv_xgb.fit(X_train,y_train)\n",
    "y_pred = classifier_tv_xgb.predict(X_val)\n",
    "yt_pred = classifier_tv_xgb.predict(X_train)\n",
    "cm = confusion_matrix(y_val,y_pred)\n",
    "print(f'Confusion Matrix :\\n {cm}\\n')\n",
    "print(f'Val Set Accuracy Score :\\n {accuracy_score(y_val,y_pred)}\\n')\n",
    "print(f'Train Set Accuracy Score :\\n {accuracy_score(y_train,yt_pred)}\\n')\n",
    "print(f'Classification Report :\\n {classification_report(y_val,y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__XGB CountVectorizer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 40s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      " [[222   9 115]\n",
      " [  9 275 291]\n",
      " [ 39 175 752]]\n",
      "\n",
      "Test Set Accuracy Score :\n",
      " 0.661897191308956\n",
      "\n",
      "Train Set Accuracy Score :\n",
      " 0.9777272727272728\n",
      "\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        bad       0.82      0.64      0.72       346\n",
      "  excellent       0.60      0.48      0.53       575\n",
      "       good       0.65      0.78      0.71       966\n",
      "\n",
      "avg / total       0.67      0.66      0.66      1887\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%time classifier_cv_xgb.fit(X_train,y_train)\n",
    "y_pred = classifier_cv_xgb.predict(X_val)\n",
    "yt_pred = classifier_cv_xgb.predict(X_train)\n",
    "cm = confusion_matrix(y_val,y_pred)\n",
    "print(f'Confusion Matrix :\\n {cm}\\n')\n",
    "print(f'Val Set Accuracy Score :\\n {accuracy_score(y_val,y_pred)}\\n')\n",
    "print(f'Train Set Accuracy Score :\\n {accuracy_score(y_train,yt_pred)}\\n')\n",
    "print(f'Classification Report :\\n {classification_report(y_val,y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_svd_scale' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-84-3b064da926f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"best\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train_svd_scale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Learning Curves (SVM, RBF kernel, $\\gamma=0.001$)\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m# SVC is more expensive so we do a lower number of CV iterations:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_svd_scale' is not defined"
     ]
    }
   ],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    val_scores_mean = np.mean(val_scores, axis=1)\n",
    "    val_scores_std = np.std(val_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, val_scores_mean - val_scores_std,\n",
    "                     val_scores_mean + val_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, val_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "X, y = X_train_svd_scale, y_train\n",
    "title = \"Learning Curves (SVM, RBF kernel, $\\gamma=0.001$)\"\n",
    "# SVC is more expensive so we do a lower number of CV iterations:\n",
    "cv = ShuffleSplit(n_splits=10, val_size=0.2, random_state=0)\n",
    "estimator = SVC(gamma=0.001)\n",
    "plot_learning_curve(estimator, title, X, y, (0.5, 1.01), cv=cv, n_jobs=4)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([(‘vectorizer’, CountVectorizer()),\n",
    " (‘tfidf’, TfidfTransformer()),\n",
    " (‘clf’, OneVsRestClassifier(LinearSVC(class_weight=”balanced”)))])\n",
    "#the class_weight=\"balanced\" option tries to remove the biasedness of model towards majority sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paramater selection\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "parameters = {'vectorizer__ngram_range': [(1, 1), (1, 2),(2,2)],\n",
    "               'tfidf__use_idf': (True, False)}\n",
    "gs_clf_svm = GridSearchCV(model, parameters, n_jobs=-1)\n",
    "gs_clf_svm = gs_clf_svm.fit(X, y)\n",
    "print(gs_clf_svm.best_score_)\n",
    "print(gs_clf_svm.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing the final pipeline using the selected parameters\n",
    "model = Pipeline([('vectorizer', CountVectorizer(ngram_range=(1,2))),\n",
    "    ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "    ('clf', OneVsRestClassifier(LinearSVC(class_weight=\"balanced\")))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model with training data\n",
    "model.fit(X_train, y_train)\n",
    "#evaluation on test data\n",
    "pred = model.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "confusion_matrix(pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.externals import joblib\n",
    "model = joblib.load('model_question_topic.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "model = joblib.load('model_question_topic.pkl')\n",
    "question = input()\n",
    "model.predict([question])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class_names = ['bad','good','excellent']\n",
    "\n",
    "losses = []\n",
    "auc = []\n",
    "\n",
    "for class_name in class_names:\n",
    "    #call the labels one column at a time so we can run the classifier on them\n",
    "    train_target = y_train[class_name]\n",
    "    test_target = y_test[class_name]\n",
    "    classifier = LogisticRegression(solver='sag', C=10)\n",
    "\n",
    "    cv_loss = np.mean(cross_val_score(classifier, X_train_word_features, train_target, cv=5, scoring='neg_log_loss'))\n",
    "    losses.append(cv_loss)\n",
    "    print('CV Log_loss score for class {} is {}'.format(class_name, cv_loss))\n",
    "\n",
    "    cv_score = np.mean(cross_val_score(classifier, X_train_word_features, train_target, cv=5, scoring='accuracy'))\n",
    "    print('CV Accuracy score for class {} is {}'.format(class_name, cv_score))\n",
    "    \n",
    "    classifier.fit(X_train_word_features, train_target)\n",
    "    y_pred = classifier.predict(test_features)\n",
    "    y_pred_prob = classifier.predict_proba(test_features)[:, 1]\n",
    "    auc_score = metrics.roc_auc_score(test_target, y_pred_prob)\n",
    "    auc.append(auc_score)\n",
    "    print(\"CV ROC_AUC score {}\\n\".format(auc_score))\n",
    "    \n",
    "    print(confusion_matrix(test_target, y_pred))\n",
    "    print(classification_report(test_target, y_pred))\n",
    "\n",
    "print('Total average CV Log_loss score is {}'.format(np.mean(losses)))\n",
    "print('Total average CV ROC_AUC score is {}'.format(np.mean(auc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviewid</th>\n",
       "      <th>Hotelid</th>\n",
       "      <th>userid</th>\n",
       "      <th>Date</th>\n",
       "      <th>reviewtext</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 127</th>\n",
       "      <th>Unnamed: 128</th>\n",
       "      <th>Unnamed: 129</th>\n",
       "      <th>Unnamed: 130</th>\n",
       "      <th>Unnamed: 131</th>\n",
       "      <th>Unnamed: 132</th>\n",
       "      <th>Unnamed: 133</th>\n",
       "      <th>Unnamed: 134</th>\n",
       "      <th>Unnamed: 135</th>\n",
       "      <th>Unnamed: 136</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Review_11001</td>\n",
       "      <td>hotel_101</td>\n",
       "      <td>hotel_2225</td>\n",
       "      <td>Dec 13, 2008</td>\n",
       "      <td>Just An Average stay This was just an average ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Review_11002</td>\n",
       "      <td>hotel_101</td>\n",
       "      <td>hotel_5079</td>\n",
       "      <td>Dec 2, 2008</td>\n",
       "      <td>go elsewhere The place is hugely overpriced an...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Review_11003</td>\n",
       "      <td>hotel_101</td>\n",
       "      <td>hotel_8440</td>\n",
       "      <td>Nov 18, 2008</td>\n",
       "      <td>I Won't Go Back I stayed at the hotel 11/14/08...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Review_11004</td>\n",
       "      <td>hotel_101</td>\n",
       "      <td>hotel_4592</td>\n",
       "      <td>Oct 19, 2008</td>\n",
       "      <td>Good weekend stay My wife and I stay here quit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Review_11005</td>\n",
       "      <td>hotel_101</td>\n",
       "      <td>hotel_5901</td>\n",
       "      <td>Oct 13, 2008</td>\n",
       "      <td>Great airport stay Lovely indoor pool area, lo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 137 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Reviewid    Hotelid      userid          Date  \\\n",
       "0  Review_11001  hotel_101  hotel_2225  Dec 13, 2008   \n",
       "1  Review_11002  hotel_101  hotel_5079   Dec 2, 2008   \n",
       "2  Review_11003  hotel_101  hotel_8440  Nov 18, 2008   \n",
       "3  Review_11004  hotel_101  hotel_4592  Oct 19, 2008   \n",
       "4  Review_11005  hotel_101  hotel_5901  Oct 13, 2008   \n",
       "\n",
       "                                          reviewtext  Unnamed: 5  Unnamed: 6  \\\n",
       "0  Just An Average stay This was just an average ...         NaN         NaN   \n",
       "1  go elsewhere The place is hugely overpriced an...         NaN         NaN   \n",
       "2  I Won't Go Back I stayed at the hotel 11/14/08...         NaN         NaN   \n",
       "3  Good weekend stay My wife and I stay here quit...         NaN         NaN   \n",
       "4  Great airport stay Lovely indoor pool area, lo...         NaN         NaN   \n",
       "\n",
       "   Unnamed: 7  Unnamed: 8  Unnamed: 9      ...       Unnamed: 127  \\\n",
       "0         NaN         NaN         NaN      ...                NaN   \n",
       "1         NaN         NaN         NaN      ...                NaN   \n",
       "2         NaN         NaN         NaN      ...                NaN   \n",
       "3         NaN         NaN         NaN      ...                NaN   \n",
       "4         NaN         NaN         NaN      ...                NaN   \n",
       "\n",
       "   Unnamed: 128  Unnamed: 129  Unnamed: 130  Unnamed: 131  Unnamed: 132  \\\n",
       "0           NaN           NaN           NaN           NaN           NaN   \n",
       "1           NaN           NaN           NaN           NaN           NaN   \n",
       "2           NaN           NaN           NaN           NaN           NaN   \n",
       "3           NaN           NaN           NaN           NaN           NaN   \n",
       "4           NaN           NaN           NaN           NaN           NaN   \n",
       "\n",
       "   Unnamed: 133  Unnamed: 134  Unnamed: 135  Unnamed: 136  \n",
       "0           NaN           NaN           NaN           NaN  \n",
       "1           NaN           NaN           NaN           NaN  \n",
       "2           NaN           NaN           NaN           NaN  \n",
       "3           NaN           NaN           NaN           NaN  \n",
       "4           NaN           NaN           NaN           NaN  \n",
       "\n",
       "[5 rows x 137 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata = pd.read_csv(\"C:/Users/sohan/Downloads/PHD_ML/Data/Test.csv\")\n",
    "testdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Reviewid', 'Hotelid', 'userid', 'Date', 'reviewtext', 'Unnamed: 5',\n",
       "       'Unnamed: 6', 'Unnamed: 7', 'Unnamed: 8', 'Unnamed: 9',\n",
       "       ...\n",
       "       'Unnamed: 127', 'Unnamed: 128', 'Unnamed: 129', 'Unnamed: 130',\n",
       "       'Unnamed: 131', 'Unnamed: 132', 'Unnamed: 133', 'Unnamed: 134',\n",
       "       'Unnamed: 135', 'Unnamed: 136'],\n",
       "      dtype='object', length=137)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata=testdata[['Reviewid', 'Hotelid', 'userid', 'Date', 'reviewtext']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviewid</th>\n",
       "      <th>Hotelid</th>\n",
       "      <th>userid</th>\n",
       "      <th>Date</th>\n",
       "      <th>reviewtext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3468</td>\n",
       "      <td>3468</td>\n",
       "      <td>3468</td>\n",
       "      <td>3468</td>\n",
       "      <td>3468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>3468</td>\n",
       "      <td>100</td>\n",
       "      <td>3403</td>\n",
       "      <td>365</td>\n",
       "      <td>3468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Review_13513</td>\n",
       "      <td>hotel_188</td>\n",
       "      <td>hotel_3316</td>\n",
       "      <td>Mar 28, 2008</td>\n",
       "      <td>Heres the facts from our trip...       2 Queen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>161</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Reviewid    Hotelid      userid          Date  \\\n",
       "count           3468       3468        3468          3468   \n",
       "unique          3468        100        3403           365   \n",
       "top     Review_13513  hotel_188  hotel_3316  Mar 28, 2008   \n",
       "freq               1        161           3            23   \n",
       "\n",
       "                                               reviewtext  \n",
       "count                                                3468  \n",
       "unique                                               3468  \n",
       "top     Heres the facts from our trip...       2 Queen...  \n",
       "freq                                                    1  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata_review=testdata.reviewtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Sanitising text__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "REPLACE_BY_SPACE_RE = re.compile(\"[/(){}\\[\\]\\|@,;'~]\")\n",
    "BAD_SYMBOLS_RE = re.compile('[^a-zA-Z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english')+ list(ENGLISH_STOP_WORDS)+list(STOP_WORDS))\n",
    "\n",
    "def cleaning_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = textacy.preprocess.replace_numbers(text, replace_with=\" \")\n",
    "    text = textacy.preprocess.replace_emails(text, replace_with=\" \")\n",
    "    text = textacy.preprocess.replace_currency_symbols(text, replace_with=\" \")\n",
    "    text = re.sub(r\"\\b[s][h][o][w][rR]\\S+\",\"\", text)# replacing string \"showreview\" with \"\"\n",
    "    text = re.sub(r\"\\b[f][u][l][l][']\\S+\",\"\", text)# replacing string \"full\" with \"\"\n",
    "    text = re.sub(r\"(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?\",\"\", text)# replacing url strings  with \"\"\n",
    "    text = re.sub(u'[⺀-⺙⺛-⻳⼀-⿕々〇〡-〩〸-〺〻㐀-䶵一-鿃豈-鶴侮-頻並-龎]',\"\", text)\n",
    "    text = re.sub(\"(\\w+\\s+\\s+\\w+)$\",\"\", text)\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub('', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub(' ', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwords from text\n",
    "    return text\n",
    "\n",
    "def spacy_tokenizer(sentence):\n",
    "    mytokens = parser(sentence)\n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "    mytokens = [ word for word in mytokens if word not in stopwords]\n",
    "    return mytokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = classifier_tv_lr.predict(X_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
